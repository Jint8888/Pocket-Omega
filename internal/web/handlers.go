package web

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"strings"
	"time"

	"github.com/pocketomega/pocket-omega/internal/agent"
	"github.com/pocketomega/pocket-omega/internal/core"
	"github.com/pocketomega/pocket-omega/internal/llm"
	"github.com/pocketomega/pocket-omega/internal/prompt"
	"github.com/pocketomega/pocket-omega/internal/session"
	"github.com/pocketomega/pocket-omega/internal/thinking"
	"github.com/pocketomega/pocket-omega/internal/tool"
)

const (
	maxRequestBody  = 1 << 20         // 1MB max request body
	maxMessageRunes = 8000            // max user message length in runes
	chatTimeout     = 5 * time.Minute // global timeout for chat flow
	agentTimeout    = 5 * time.Minute // global timeout for agent flow
)

// ‚îÄ‚îÄ SSE Writer ‚îÄ‚îÄ

// sseWriter wraps an http.ResponseWriter with SSE event writing and
// client disconnect detection. Shared by both Chat and Agent handlers.
type sseWriter struct {
	w       http.ResponseWriter
	flusher http.Flusher
	ctx     context.Context
}

// newSSEWriter prepares SSE headers and returns a writer.
// Returns nil if streaming is not supported.
func newSSEWriter(w http.ResponseWriter, r *http.Request) *sseWriter {
	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, "Streaming not supported", http.StatusInternalServerError)
		return nil
	}

	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")
	w.Header().Set("X-Accel-Buffering", "no")

	return &sseWriter{w: w, flusher: flusher, ctx: r.Context()}
}

// Send writes an SSE event. Returns false if the client has disconnected.
func (s *sseWriter) Send(event string, data interface{}) bool {
	select {
	case <-s.ctx.Done():
		return false
	default:
	}
	jsonBytes, err := json.Marshal(data)
	if err != nil {
		log.Printf("[SSE] JSON marshal error: %v", err)
		return false
	}
	if _, err := fmt.Fprintf(s.w, "event: %s\ndata: %s\n\n", event, string(jsonBytes)); err != nil {
		log.Printf("[SSE] Write error (client disconnected?): %v", err)
		return false
	}
	s.flusher.Flush()
	return true
}

// ‚îÄ‚îÄ Shared Solution Formatter ‚îÄ‚îÄ

// formatSolutionPromptDefault is the fallback system prompt for the solution
// formatting step used when no loader is available or answer_style.md is absent.
const formatSolutionPromptDefault = `‰Ω†ÊòØ‰∏Ä‰∏™Á≠îÊ°àÊï¥ÁêÜÂä©Êâã„ÄÇÂ∞ÜÊé®ÁêÜÁªìËÆ∫Êï¥ÁêÜ‰∏∫Ê∏ÖÊô∞„ÄÅÂèãÂ•ΩÁöÑÊúÄÁªàÂõûÁ≠î„ÄÇ

## È£éÊ†ºÊåáÂçó
- Ê≠•È™§/ÊñπÊ°àÁî®ÊúâÂ∫èÂàóË°®ÔºåË¶ÅÁÇπÁî®Êó†Â∫èÂàóË°®
- ÈáçÁÇπÂÖ≥ÈîÆËØçÁî® **Âä†Á≤ó**
- ‰ª£Á†Å/ÂëΩ‰ª§Áî®‰ª£Á†ÅÂùó
- ‰øùÊåÅËØ≠Ë®Ä‰∏éÁî®Êà∑‰∏ÄËá¥Ôºà‰∏≠ÊñáÈóÆÁî®‰∏≠ÊñáÁ≠îÔºâ
- ‰∏çË¶ÅÊ∑ªÂä†"‰ª•‰∏ãÊòØÁ≠îÊ°à"‰πãÁ±ªÁöÑÂâçÁºÄÔºåÁõ¥Êé•‰ΩúÁ≠î
- Â¶ÇÊûúÂéüÂßãÁªìËÆ∫Â∑≤Ë∂≥Â§üÂ•ΩÔºåÁõ¥Êé•‰øùÁïô‰∏çË¶ÅËøáÂ∫¶‰øÆÈ•∞

## Á§∫‰æã

Áî®Êà∑ÈóÆÈ¢òÔºö‰∏Ä‰∏™ÊàøÈó¥ÈáåÊúâ3ÁõèÁÅØÔºåÊàøÈó¥Â§ñÊúâ3‰∏™ÂºÄÂÖ≥„ÄÇ‰Ω†Âè™ËÉΩËøõÂÖ•ÊàøÈó¥‰∏ÄÊ¨°„ÄÇÂ¶Ç‰ΩïÁ°ÆÂÆöÂì™‰∏™ÂºÄÂÖ≥ÊéßÂà∂Âì™ÁõèÁÅØÔºü

Êï¥ÁêÜÂêéÁöÑÁ≠îÊ°àÔºö

üí° **Ê†∏ÂøÉÊÄùË∑ØÔºö** Âà©Áî®ÁÅØÊ≥°ÈÄöÁîµÂêéÁöÑ **ÁÉ≠ÊÉ∞ÊÄß** ÂºïÂÖ•Á¨¨‰∏â‰∏™Âà§Êñ≠Áª¥Â∫¶„ÄÇ

üìù **Êìç‰ΩúÊ≠•È™§Ôºö**

1. **ÊâìÂºÄÂºÄÂÖ≥ 1**Ôºå‰øùÊåÅÁ∫¶ 5 ÂàÜÈíüÔºåËÆ©ÁÅØÊ≥°ÂÖÖÂàÜÂèëÁÉ≠
2. **ÂÖ≥Èó≠ÂºÄÂÖ≥ 1**ÔºåÁ´ãÂç≥ **ÊâìÂºÄÂºÄÂÖ≥ 2**
3. **ËøõÂÖ•ÊàøÈó¥**ÔºåËßÇÂØüÂπ∂Ëß¶Êë∏ÁÅØÊ≥°

üîç **Âà§Êñ≠ÊñπÊ≥ïÔºö**

- üí° **‰∫ÆÁùÄÁöÑÁÅØ** ‚Üí ÂºÄÂÖ≥ 2 ÊéßÂà∂ÔºàÂΩìÂâçÈÄöÁîµÔºâ
- üî• **‰∏ç‰∫Æ‰ΩÜÊ∏©ÁÉ≠** ‚Üí ÂºÄÂÖ≥ 1 ÊéßÂà∂ÔºàÂàöÊñ≠ÁîµÔºå‰ΩôÊ∏©Â∞öÂú®Ôºâ
- ‚ùÑÔ∏è **‰∏ç‰∫Æ‰∏îÂÜ∞Âáâ** ‚Üí ÂºÄÂÖ≥ 3 ÊéßÂà∂Ôºà‰ªéÊú™ÈÄöÁîµÔºâ

‚úÖ ÂÖ≥ÈîÆÂú®‰∫éÂà©Áî®ÁÅØÊ≥°ÁöÑÁÉ≠ÊÉ∞ÊÄßÔºåÂ∞Ü"Âè™ËÉΩËøõ‰∏ÄÊ¨°"ÁöÑ‰∏§ÊÄÅÂà§Êñ≠Ôºà‰∫Æ/ÁÅ≠ÔºâÊâ©Â±ï‰∏∫‰∏âÊÄÅÂà§Êñ≠Ôºà‰∫Æ/ÊöóÁÉ≠/ÊöóÂÜ∑Ôºâ„ÄÇ`

// buildFormatPrompt assembles the system prompt for the solution formatting step.
// Uses answer_style.md from loader (L2+L3) when available.
func buildFormatPrompt(loader *prompt.PromptLoader) string {
	if loader == nil {
		return formatSolutionPromptDefault
	}

	style := loader.Load("answer_style.md")
	if style == "" {
		return formatSolutionPromptDefault
	}

	// L2 style + L3 user rules
	var sb strings.Builder
	sb.WriteString("‰Ω†ÊòØ‰∏Ä‰∏™Á≠îÊ°àÊï¥ÁêÜÂä©Êâã„ÄÇÂ∞ÜÊé®ÁêÜÁªìËÆ∫Êï¥ÁêÜ‰∏∫Ê∏ÖÊô∞„ÄÅÂèãÂ•ΩÁöÑÊúÄÁªàÂõûÁ≠î„ÄÇ\n\n")
	sb.WriteString(style)
	if rules := loader.LoadUserRules(); rules != "" {
		sb.WriteString("\n\n## Áî®Êà∑Ëá™ÂÆö‰πâËßÑÂàô\n")
		sb.WriteString(rules)
	}
	return sb.String()
}

// formatSolution makes a lightweight LLM call to clean and organize
// a raw conclusion into a well-structured, user-facing answer.
// Shared by both ChatHandler and AgentHandler.
func formatSolution(ctx context.Context, provider llm.LLMProvider, loader *prompt.PromptLoader, problem, rawSolution string) (string, error) {
	userPrompt := fmt.Sprintf("Áî®Êà∑ÈóÆÈ¢òÔºö%s\n\nÂéüÂßãÊé®ÁêÜÁªìËÆ∫Ôºö\n%s\n\nËØ∑Êï¥ÁêÜ‰∏∫ÊúÄÁªàÁ≠îÊ°àÔºö", problem, rawSolution)

	resp, err := provider.CallLLM(ctx, []llm.Message{
		{Role: llm.RoleSystem, Content: buildFormatPrompt(loader)},
		{Role: llm.RoleUser, Content: userPrompt},
	})
	if err != nil {
		return "", fmt.Errorf("format LLM call failed: %w", err)
	}

	formatted := strings.TrimSpace(resp.Content)
	if formatted == "" {
		return "", fmt.Errorf("format returned empty response")
	}

	log.Printf("[Format] Formatted solution: %d -> %d chars", len(rawSolution), len(formatted))
	return formatted, nil
}

// ‚îÄ‚îÄ SSE Event Types ‚îÄ‚îÄ

type sseThoughtEvent struct {
	ThoughtNumber   int    `json:"thought_number"`
	CurrentThinking string `json:"current_thinking"`
	PlanText        string `json:"plan_text,omitempty"`
}

type sseDoneEvent struct {
	Solution string `json:"solution"`
}

type sseErrorEvent struct {
	Error string `json:"error"`
}

// ‚îÄ‚îÄ Chat Handler ‚îÄ‚îÄ

// ChatHandler handles chat requests and runs the CoT flow.
type ChatHandler struct {
	llmProvider         llm.LLMProvider
	maxRetries          int
	contextWindowTokens int
	sessionStore        *session.Store
	loader              *prompt.PromptLoader
}

// NewChatHandler creates a new handler with the given LLM provider.
// loader is optional (nil is valid) ‚Äî falls back to hardcoded defaults.
func NewChatHandler(provider llm.LLMProvider, maxRetries int, contextWindowTokens int, store *session.Store, loader *prompt.PromptLoader) *ChatHandler {
	return &ChatHandler{
		llmProvider:         provider,
		maxRetries:          maxRetries,
		contextWindowTokens: contextWindowTokens,
		sessionStore:        store,
		loader:              loader,
	}
}

// HandleChat processes chat POST requests using SSE streaming.
func (h *ChatHandler) HandleChat(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	r.Body = http.MaxBytesReader(w, r.Body, maxRequestBody)

	userMsg := strings.TrimSpace(r.FormValue("message"))
	if userMsg == "" {
		http.Error(w, "Empty message", http.StatusBadRequest)
		return
	}
	if len([]rune(userMsg)) > maxMessageRunes {
		http.Error(w, "Message too long", http.StatusRequestEntityTooLarge)
		return
	}

	log.Printf("[Chat] Received: %s", userMsg)

	// Session history lookup
	sessionID := strings.TrimSpace(r.FormValue("session_id"))
	var historyMsgs []llm.Message
	if sessionID != "" && h.sessionStore != nil {
		turns := h.sessionStore.GetHistory(sessionID)
		// Allocate 50% of context window (in chars) to chat history.
		// More generous than Agent's 30% since Chat has no tool output overhead.
		// When contextWindowTokens is 0 (unknown), budget is 0 (no cap).
		budget := h.contextWindowTokens * 2 * 50 / 100
		historyMsgs = session.ToMessages(turns, budget)
	}

	sse := newSSEWriter(w, r)
	if sse == nil {
		return
	}

	// Global timeout for the chat flow
	ctx, cancel := context.WithTimeout(r.Context(), chatTimeout)
	defer cancel()

	// Build and run the CoT flow with streaming callback
	flow := thinking.BuildFlow(h.llmProvider, h.maxRetries)
	state := &thinking.ThinkingState{
		Problem:             userMsg,
		ConversationHistory: historyMsgs,
		OnThoughtComplete: func(thought thinking.ThoughtData) {
			sse.Send("thought", sseThoughtEvent{
				ThoughtNumber:   thought.ThoughtNumber,
				CurrentThinking: strings.TrimSpace(thought.CurrentThinking),
				PlanText:        thinking.FormatPlan(thought.Planning, 0),
			})
		},
	}
	flow.Run(ctx, state)

	solution := strings.TrimSpace(state.Solution)
	if solution == "" {
		solution = "Êä±Ê≠âÔºåÊú™ËÉΩÁîüÊàêÂõûÁ≠î„ÄÇËØ∑ÈáçËØï„ÄÇ"
	} else {
		// ChatHandler uses ThinkingFlow which has no AnswerNode ‚Äî the raw CoT
		// conclusion needs a formatting pass to produce a polished user-facing answer.
		// (AgentHandler skips this step because its AnswerNode already synthesizes
		// the final response with an LLM call, making a second pass redundant.)
		formatted, err := formatSolution(ctx, h.llmProvider, h.loader, userMsg, solution)
		if err != nil {
			log.Printf("[Format] Formatting failed, using raw solution: %v", err)
		} else {
			solution = formatted
		}
	}

	sse.Send("done", sseDoneEvent{Solution: solution})
	log.Printf("[Chat] Done: %d thoughts, solution %d chars", len(state.Thoughts), len(solution))

	// Persist this turn to session history
	if sessionID != "" && h.sessionStore != nil {
		h.sessionStore.AppendTurn(sessionID, session.Turn{
			UserMsg:   userMsg,
			Assistant: solution,
			IsAgent:   false,
		})
	}
}

// ‚îÄ‚îÄ Agent Handler (Phase 2) ‚îÄ‚îÄ

// AgentHandlerOptions groups all configuration for AgentHandler.
// Use this instead of positional parameters to keep NewAgentHandler maintainable
// as new options are added over time.
type AgentHandlerOptions struct {
	Provider            llm.LLMProvider
	Registry            *tool.Registry
	WorkspaceDir        string
	ExecLogger          *agent.ExecLogger
	ThinkingMode        string
	ToolCallMode        string
	ContextWindowTokens int
	Store               *session.Store
	Loader              *prompt.PromptLoader // optional ‚Äî falls back to hardcoded defaults
	OSName              string               // e.g. "Windows" ‚Äî for runtime info line
	ShellCmd            string               // e.g. "cmd.exe /c" ‚Äî for runtime info line
	ModelName           string               // e.g. "gemini-2.5-pro" ‚Äî for runtime info line
}

// AgentHandler handles agent requests with tool usage capability.
type AgentHandler struct {
	llmProvider         llm.LLMProvider
	agentFlow           core.Workflow[agent.AgentState]
	toolRegistry        *tool.Registry
	workspaceDir        string
	execLogger          *agent.ExecLogger
	thinkingMode        string
	toolCallMode        string
	contextWindowTokens int
	sessionStore        *session.Store
	loader              *prompt.PromptLoader
	osName              string
	shellCmd            string
	modelName           string
}

// NewAgentHandler creates a new agent handler from AgentHandlerOptions.
func NewAgentHandler(opts AgentHandlerOptions) *AgentHandler {
	return &AgentHandler{
		llmProvider:         opts.Provider,
		agentFlow:           agent.BuildAgentFlow(opts.Provider, opts.Registry, opts.ThinkingMode, opts.Loader),
		toolRegistry:        opts.Registry,
		workspaceDir:        opts.WorkspaceDir,
		execLogger:          opts.ExecLogger,
		thinkingMode:        opts.ThinkingMode,
		toolCallMode:        opts.ToolCallMode,
		contextWindowTokens: opts.ContextWindowTokens,
		sessionStore:        opts.Store,
		loader:              opts.Loader,
		osName:              opts.OSName,
		shellCmd:            opts.ShellCmd,
		modelName:           opts.ModelName,
	}
}

// HandleAgent processes agent requests using SSE streaming with tool calls.
func (h *AgentHandler) HandleAgent(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	r.Body = http.MaxBytesReader(w, r.Body, maxRequestBody)

	userMsg := strings.TrimSpace(r.FormValue("message"))
	if userMsg == "" {
		http.Error(w, "Empty message", http.StatusBadRequest)
		return
	}
	if len([]rune(userMsg)) > maxMessageRunes {
		http.Error(w, "Message too long", http.StatusRequestEntityTooLarge)
		return
	}

	log.Printf("[Agent] Received: %s", userMsg)

	// Session history lookup
	sessionID := strings.TrimSpace(r.FormValue("session_id"))
	var historyPrefix string
	if sessionID != "" && h.sessionStore != nil {
		turns := h.sessionStore.GetHistory(sessionID)
		// allocate 30% of context window (in chars) to conversation history
		budget := h.contextWindowTokens * 2 * 30 / 100
		historyPrefix = session.ToProblemPrefix(turns, budget)
	}

	sse := newSSEWriter(w, r)
	if sse == nil {
		return
	}

	// Global timeout for the entire agent flow
	ctx, cancel := context.WithTimeout(r.Context(), agentTimeout)
	defer cancel()

	// Send immediate status so user sees instant feedback
	sse.Send("status", map[string]string{"message": "ü§î Ê≠£Âú®ÂàÜÊûêÈóÆÈ¢ò..."})

	// Start execution log session
	if h.execLogger != nil {
		h.execLogger.StartSession(userMsg)
	}

	// Build agent state with SSE callback
	state := &agent.AgentState{
		Problem:             userMsg,
		ConversationHistory: historyPrefix,
		WorkspaceDir:        h.workspaceDir,
		ToolRegistry:        h.toolRegistry,
		ThinkingMode:        h.thinkingMode,
		ToolCallMode:        h.toolCallMode,
		ContextWindowTokens: h.contextWindowTokens,
		OSName:              h.osName,
		ShellCmd:            h.shellCmd,
		ModelName:           h.modelName,
		OnStepComplete: func(step agent.StepRecord) {
			// Write to execution log
			if h.execLogger != nil {
				h.execLogger.LogStep(step)
			}
			switch step.Type {
			case "decide":
				sse.Send("step", step)
			case "tool":
				sse.Send("tool", step)
			case "think":
				sse.Send("step", step)
			}
		},
		OnStreamChunk: func(chunk string) {
			sse.Send("chunk", map[string]string{"text": chunk})
		},
	}

	// Run the agent flow with timeout context
	h.agentFlow.Run(ctx, state)

	// AnswerNode already synthesizes a polished answer with LLM.
	// Skip formatSolution here to avoid a redundant LLM round-trip
	// that adds 3-5s of latency with no visible benefit.
	solution := strings.TrimSpace(state.Solution)
	if solution == "" {
		solution = "Êä±Ê≠âÔºåÊú™ËÉΩÁîüÊàêÂõûÁ≠î„ÄÇËØ∑ÈáçËØï„ÄÇ"
	}

	sse.Send("done", sseDoneEvent{Solution: solution})
	log.Printf("[Agent] Done: %d steps, solution %d chars", len(state.StepHistory), len(solution))

	// Write execution log summary
	if h.execLogger != nil {
		h.execLogger.EndSession(state)
	}

	// Persist this turn to session history
	if sessionID != "" && h.sessionStore != nil {
		h.sessionStore.AppendTurn(sessionID, session.Turn{
			UserMsg:   userMsg,
			Assistant: solution,
			IsAgent:   true,
		})
	}
}
